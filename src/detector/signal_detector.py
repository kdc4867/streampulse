import time
import schedule
import duckdb
import psycopg2
import os
import requests
import json
import numpy as np
from datetime import datetime, timedelta

# === ì„¤ì • ===
DUCK_PATH = os.getenv("DB_PATH", "data/analytics.db")
PG_DSN = f"host=postgres dbname={os.getenv('POSTGRES_DB', 'streampulse_meta')} user={os.getenv('POSTGRES_USER', 'user')} password={os.getenv('POSTGRES_PASSWORD', 'password')}"
AGENT_URL = "http://agent:8000/analyze"

# V3 í™•ì • íŒŒë¼ë¯¸í„°
MIN_ABSOLUTE_DELTA = 1000   # ìµœì†Œ ì¦ê°€ëŸ‰ (í•˜í•œì„ )
DELTA_RATIO = 0.3           # ë™ì  ë¸íƒ€ ë¹„ìœ¨ (30%)
GROWTH_THRESHOLD = 1.5      # 1.5ë°° (ë‹¨ê¸° ê¸‰ë“±)
SEASONAL_THRESHOLD = 1.2    # 1.2ë°° (ì¥ê¸° ì¶”ì„¸ ëŒ€ë¹„)
COOLDOWN_MINUTES = 30       # ì¬ì•Œë¦¼ ê¸ˆì§€

def get_pg_conn():
    return psycopg2.connect(PG_DSN)

def init_db():
    """Postgres í…Œì´ë¸” ì´ˆê¸°í™”"""
    try:
        conn = get_pg_conn()
        cur = conn.cursor()
        # ì´ë²¤íŠ¸ ê¸°ë¡ í…Œì´ë¸”
        cur.execute("""
            CREATE TABLE IF NOT EXISTS signal_events (
                event_id SERIAL PRIMARY KEY,
                created_at TIMESTAMP DEFAULT NOW(),
                platform VARCHAR(20),
                category_name VARCHAR(100),
                event_type VARCHAR(50), 
                growth_rate FLOAT,
                cause_detail JSONB
            );
            CREATE INDEX IF NOT EXISTS idx_cool ON signal_events (platform, category_name, created_at);
        """)
        conn.commit()
        conn.close()
    except Exception as e:
        print(f"[Detector] DB Init Fail: {e}")

def check_cooldown(platform, category):
    """ì¿¨íƒ€ì„ ì²´í¬ (30ë¶„)"""
    try:
        conn = get_pg_conn()
        cur = conn.cursor()
        cur.execute("""
            SELECT 1 FROM signal_events
            WHERE platform = %s AND category_name = %s
              AND created_at >= NOW() - INTERVAL '%s minutes'
        """, (platform, category, COOLDOWN_MINUTES))
        exists = cur.fetchone()
        conn.close()
        return exists is not None
    except Exception:
        return False

def calculate_contribution(cur_view, past_view, cur_top_json, past_top_json):
    """
    [ì›ì¸ ë¶„ì„ í•µì‹¬ ë¡œì§] ì¦ê°€ë¶„ ê¸°ì—¬ìœ¨(Incremental Contribution) ê³„ì‚°
    Formula: (Top5_Current_Sum - Top5_Past_Sum) / (Current_Total - Past_Total)
    """
    try:
        # JSON íŒŒì‹± ë° Top5 í•©ê³„ ê³„ì‚°
        cur_list = json.loads(cur_top_json) if cur_top_json else []
        past_list = json.loads(past_top_json) if past_top_json else []
        
        cur_top_sum = sum([item.get('viewers', 0) for item in cur_list])
        past_top_sum = sum([item.get('viewers', 0) for item in past_list])
        
        # ë¸íƒ€ ê³„ì‚°
        total_delta = cur_view - past_view
        top_delta = cur_top_sum - past_top_sum
        
        if total_delta <= 0: return "STRUCTURE_ISSUE", 0.0, cur_list # í•˜ë½/ë³´í•©ì€ êµ¬ì¡° ì´ìŠˆë¡œ ì¹¨

        contribution = top_delta / total_delta
        
        # ê¸°ì—¬ìœ¨ì´ 50% ë„˜ìœ¼ë©´ ì¸ë¬¼ ì´ìŠˆ
        if contribution >= 0.5:
            return "PERSON_ISSUE", contribution, cur_list
        else:
            return "STRUCTURE_ISSUE", contribution, cur_list
            
    except Exception as e:
        print(f"[Calc Error] {e}")
        return "STRUCTURE_ISSUE", 0.0, []

def detect_spikes():
    print(f"\n[Detector] ğŸ” V3 ë¡œì§ ë¶„ì„ ì‹œì‘ ({time.strftime('%H:%M:%S')})")
    
    try:
        duck = duckdb.connect(DUCK_PATH, read_only=True)
        
        # 1. ìµœì‹  ë°ì´í„° ì‹œì  í™•ì¸
        last_row = duck.execute("SELECT MAX(ts_utc) FROM traffic_category_snapshot").fetchone()
        if not last_row or not last_row[0]:
            print("[Detector] ë°ì´í„° ë¶€ì¡±.")
            return
        last_ts = last_row[0]

        # 2. V3 í•µì‹¬ ì¿¼ë¦¬ (Median, 7-Day, 24-Hour, Current í•œ ë²ˆì— ì¡°íšŒ)
        # LEAD/LAG ëŒ€ì‹  ë²”ìœ„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¡°ì¸
        query = f"""
        WITH 
        -- 1. í˜„ì¬ ë°ì´í„°
        curr AS (
            SELECT platform, category_name, viewers, top_streamers_detail, ts_utc
            FROM traffic_category_snapshot 
            WHERE ts_utc = CAST('{last_ts}' AS TIMESTAMP)
        ),
        -- 2. ë‹¨ê¸° ë² ì´ìŠ¤ë¼ì¸ (ì§ì „ 60ë¶„ ì¤‘ì•™ê°’)
        short_term AS (
            SELECT platform, category_name, MEDIAN(viewers) as median_60m, 
                   FIRST(viewers) as view_1h_ago, FIRST(top_streamers_detail) as top_1h_ago
            FROM traffic_category_snapshot
            WHERE ts_utc BETWEEN CAST('{last_ts}' AS TIMESTAMP) - INTERVAL 60 MINUTE 
                             AND CAST('{last_ts}' AS TIMESTAMP)
            GROUP BY platform, category_name
        ),
        -- 3. ì¥ê¸° ë² ì´ìŠ¤ë¼ì¸ A (7ì¼ ì „)
        seasonal_7d AS (
            SELECT platform, category_name, AVG(viewers) as avg_7d
            FROM traffic_category_snapshot
            WHERE ts_utc BETWEEN CAST('{last_ts}' AS TIMESTAMP) - INTERVAL 169 HOUR 
                             AND CAST('{last_ts}' AS TIMESTAMP) - INTERVAL 167 HOUR
            GROUP BY platform, category_name
        ),
        -- 4. ì¥ê¸° ë² ì´ìŠ¤ë¼ì¸ B (24ì‹œê°„ ì „ - Fallbackìš©)
        seasonal_24h AS (
            SELECT platform, category_name, AVG(viewers) as avg_24h
            FROM traffic_category_snapshot
            WHERE ts_utc BETWEEN CAST('{last_ts}' AS TIMESTAMP) - INTERVAL 25 HOUR 
                             AND CAST('{last_ts}' AS TIMESTAMP) - INTERVAL 23 HOUR
            GROUP BY platform, category_name
        )
        SELECT 
            c.platform, c.category_name, c.viewers,
            s.median_60m, s.view_1h_ago, s.top_1h_ago,
            d7.avg_7d, d24.avg_24h,
            c.top_streamers_detail
        FROM curr c
        LEFT JOIN short_term s ON c.platform = s.platform AND c.category_name = s.category_name
        LEFT JOIN seasonal_7d d7 ON c.platform = d7.platform AND c.category_name = d7.category_name
        LEFT JOIN seasonal_24h d24 ON c.platform = d24.platform AND c.category_name = d24.category_name
        WHERE c.viewers >= {MIN_ABSOLUTE_DELTA}
        """
        
        rows = duck.execute(query).fetchall()
        duck.close()

        alerts = 0
        for row in rows:
            platform, cat, cur_view, med_60m, view_1h, top_1h, avg_7d, avg_24h, top_cur = row
            
            # --- [Logic] Baseline ê²°ì • (ìš°ì„ ìˆœìœ„: 7ì¼ -> 24ì‹œê°„ -> í˜„ì¬ì˜ 80%) ---
            if avg_7d:
                seasonal_base = avg_7d
            elif avg_24h:
                seasonal_base = avg_24h
            else:
                seasonal_base = cur_view * 0.8 # Cold Start Fallback
            
            if not med_60m: med_60m = cur_view * 0.8
            if not view_1h: view_1h = cur_view * 0.8

            # --- [Logic] ìŠ¤íŒŒì´í¬ íŒë³„ ---
            # 1. ë™ì  ë¸íƒ€ ì„ê³„ê°’
            dynamic_delta_req = max(MIN_ABSOLUTE_DELTA, seasonal_base * DELTA_RATIO)
            actual_delta = cur_view - seasonal_base
            
            growth_ratio = cur_view / med_60m if med_60m > 0 else 0.
            # 2. ì¡°ê±´ ê²€ì‚¬
            cond_short = cur_view >= med_60m * GROWTH_THRESHOLD
            cond_season = cur_view >= seasonal_base * SEASONAL_THRESHOLD
            cond_delta = actual_delta >= dynamic_delta_req

            # [ì¶”ê°€] ëª¨ë‹ˆí„°ë§ ë¡œê·¸: 1.2ë°°ëŠ” ë„˜ì—ˆëŠ”ë° 1.5ë°°(ê¸°ì¤€)ëŠ” ì•ˆ ëœ ì• ë“¤ êµ¬ê²½í•˜ê¸°
            if growth_ratio >= 1.2 and growth_ratio < GROWTH_THRESHOLD:
                print(f"ğŸ‘€ [ê´€ì‹¬] {platform} {cat}: {cur_view}ëª… (í‰ì†Œ {int(med_60m)}ëª…, {growth_ratio:.2f}ë°°) -> ê¸°ì¤€ ë¯¸ë‹¬ë¡œ íƒˆë½")

            if cond_short and cond_season and cond_delta:
                # 3. ì¿¨íƒ€ì„
                if check_cooldown(platform, cat):
                    continue

                # 4. ì›ì¸ ë¶„ì„ (Contribution)
                cause, ratio, clue_list = calculate_contribution(cur_view, view_1h, top_cur, top_1h)
                
                print(f"ğŸš¨ [SPIKE] {platform} {cat}: {cur_view}ëª… (ê¸°ì—¬ìœ¨: {ratio*100:.1f}% -> {cause})")

                # 5. ê¸°ë¡ ë° ì—ì´ì „íŠ¸ ìš”ì²­
                event_detail = {
                    "stats": {
                        "current": cur_view, 
                        "baseline_season": int(seasonal_base),
                        "delta": int(actual_delta)
                    },
                    "clues": clue_list[:3] # ìƒìœ„ 3ëª…ë§Œ ì „ë‹¬
                }
                
                try:
                    conn = get_pg_conn()
                    cur = conn.cursor()
                    cur.execute("""
                        INSERT INTO signal_events (platform, category_name, event_type, growth_rate, cause_detail)
                        VALUES (%s, %s, %s, %s, %s)
                    """, (platform, cat, cause, round(cur_view/seasonal_base, 2), json.dumps(event_detail)))
                    conn.commit()
                    conn.close()
                    
                    # Agent í˜¸ì¶œ (Fire & Forget)
                    # requests.post(AGENT_URL, json={
                    #     "platform": platform, "category": cat,
                    #     "cause_type": cause, "stats": event_detail['stats'],
                    #     "top_clues": clue_list
                    # }, timeout=1)
                    alerts += 1
                except Exception as e:
                    print(f"âŒ Alert Fail: {e}")

        if alerts > 0:
            print(f"[Detector] {alerts}ê±´ ê°ì§€ ì™„ë£Œ.")
        else:
            print("[Detector] íŠ¹ì´ì‚¬í•­ ì—†ìŒ.")

    except Exception as e:
        print(f"[Detector] Error: {e}")

def run():
    print("ğŸ‘€ [Signal Detector V3] ê°€ë™ - (Weekly/Median/Delta)")
    time.sleep(5)
    init_db()
    schedule.every(5).minutes.do(detect_spikes)
    
    while True:
        schedule.run_pending()
        time.sleep(1)

if __name__ == "__main__":
    run()