import json
import os
import time
import logging
import psycopg2

from src.agent.graph import app as agent_app
from src.notify.telegram_bot import send_telegram_message

_default_pg_host = "postgres" if os.path.exists("/.dockerenv") else "localhost"
PG_HOST = os.getenv("POSTGRES_HOST", _default_pg_host)
PG_PORT = os.getenv("POSTGRES_PORT", "5432")
PG_DSN = (
    f"host={PG_HOST} port={PG_PORT} "
    f"dbname={os.getenv('POSTGRES_DB', 'streampulse_meta')} "
    f"user={os.getenv('POSTGRES_USER', 'user')} "
    f"password={os.getenv('POSTGRES_PASSWORD', 'password')}"
)

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
AGENT_ALERT_MODE = os.getenv("AGENT_ALERT_MODE", "confirmed")
ALERT_KEYWORDS = [
    kw.strip() for kw in os.getenv("ALERT_KEYWORDS", "íŒ¨ì¹˜,ì—…ë°ì´íŠ¸").split(",") if kw.strip()
]
RESEARCH_MIN_DELTA = int(os.getenv("RESEARCH_MIN_DELTA", "1500"))
RESEARCH_MIN_GROWTH = float(os.getenv("RESEARCH_MIN_GROWTH", "1.4"))
RESEARCH_MIN_SEASON = float(os.getenv("RESEARCH_MIN_SEASON", "1.15"))
RESEARCH_MAJOR_MIN_DELTA = int(os.getenv("RESEARCH_MAJOR_MIN_DELTA", "10000"))
RESEARCH_MAJOR_MIN_GROWTH = float(os.getenv("RESEARCH_MAJOR_MIN_GROWTH", "1.15"))
RESEARCH_MAJOR_MIN_SEASON = float(os.getenv("RESEARCH_MAJOR_MIN_SEASON", "1.10"))
ALERT_MIN_DELTA = int(os.getenv("ALERT_MIN_DELTA", "1500"))
ALERT_MIN_GROWTH = float(os.getenv("ALERT_MIN_GROWTH", "1.3"))
ALERT_MAJOR_MIN_DELTA = int(os.getenv("ALERT_MAJOR_MIN_DELTA", "3000"))
ALERT_MAJOR_MIN_GROWTH = float(os.getenv("ALERT_MAJOR_MIN_GROWTH", "1.5"))

def get_pg_conn():
    return psycopg2.connect(PG_DSN)

def fetch_pending(limit=3):
    conn = get_pg_conn()
    conn.autocommit = False
    cur = conn.cursor()
    cur.execute(
        """
        WITH cte AS (
            SELECT event_id, platform, category_name, event_type, growth_rate, cause_detail
            FROM signal_events
            WHERE analysis_status = 'PENDING'
            ORDER BY created_at ASC
            LIMIT %s
            FOR UPDATE SKIP LOCKED
        )
        UPDATE signal_events
        SET analysis_status = 'IN_PROGRESS'
        WHERE event_id IN (SELECT event_id FROM cte)
        RETURNING event_id, platform, category_name, event_type, growth_rate, cause_detail
        """,
        (limit,),
    )
    rows = cur.fetchall()
    conn.commit()
    conn.close()
    return rows

def update_event(
    event_id,
    status,
    tier,
    report,
    spike_reason,
    entity_keywords,
    cache_key,
    search_results,
    watcher_output,
    search_queries,
    analyst_output,
    editor_output,
    analysis_verdict,
    analysis_reason,
    evidence_keywords,
    event_kind,
):
    conn = get_pg_conn()
    cur = conn.cursor()
    extra = {
        "ai_report": report,
        "spike_reason": spike_reason,
        "entity_keywords": entity_keywords,
        "search_results": search_results,
        "watcher_output": watcher_output,
        "search_queries": search_queries,
        "analyst_output": analyst_output,
        "editor_output": editor_output,
        "analysis_verdict": analysis_verdict,
        "analysis_reason": analysis_reason,
        "evidence_keywords": evidence_keywords,
        "event_kind": event_kind,
    }
    cur.execute(
        """
        UPDATE signal_events
        SET analysis_status = %s,
            analysis_tier = %s,
            spike_reason = %s,
            entity_keywords = %s::jsonb,
            context_cache_key = %s,
            cause_detail = cause_detail || %s::jsonb
        WHERE event_id = %s
        """,
        (
            status,
            tier,
            spike_reason,
            json.dumps(entity_keywords, ensure_ascii=False),
            cache_key,
            json.dumps(extra, ensure_ascii=False),
            event_id,
        ),
    )
    conn.commit()
    conn.close()

def mark_failed(event_id, error):
    conn = get_pg_conn()
    cur = conn.cursor()
    cur.execute(
        """
        UPDATE signal_events
        SET analysis_status = 'FAILED',
            cause_detail = cause_detail || %s::jsonb
        WHERE event_id = %s
        """,
        (json.dumps({"ai_error": str(error)}, ensure_ascii=False), event_id),
    )
    conn.commit()
    conn.close()

def should_research(signal_level, event_type, stats):
    if event_type == "CATEGORY_ADOPTION":
        return False, "category_adoption"
    if signal_level == "SPIKE":
        return True, ""
    if signal_level != "CANDIDATE":
        return False, "not_candidate"
    growth_ratio = float(stats.get("growth_ratio") or 0.0)
    season_ratio = float(stats.get("season_ratio") or 0.0)
    actual_delta = int(stats.get("delta") or 0)
    is_major = bool(stats.get("major_category"))
    min_delta = RESEARCH_MAJOR_MIN_DELTA if is_major else RESEARCH_MIN_DELTA
    min_growth = RESEARCH_MAJOR_MIN_GROWTH if is_major else RESEARCH_MIN_GROWTH
    if growth_ratio < 1.0:
        return False, f"decreasing_trend_(ratio:{growth_ratio:.2f})"
    if actual_delta <= 0:
        return False, "delta_zero"
    if is_major:
        if actual_delta < min_delta:
            return False, "below_research_threshold"
        if growth_ratio < min_growth and season_ratio < RESEARCH_MAJOR_MIN_SEASON:
            return False, "below_research_threshold"
        return True, ""
    if actual_delta < min_delta:
        return False, "below_research_threshold"
    if growth_ratio < min_growth and season_ratio < RESEARCH_MIN_SEASON:
        return False, "below_research_threshold"
    return True, ""

def process_event(row):
    event_id, platform, category_name, event_type, growth_rate, cause_detail = row
    cause = {}
    if cause_detail:
        try:
            cause = json.loads(cause_detail) if isinstance(cause_detail, str) else cause_detail
        except Exception:
            cause = {}
    inputs = {
        "event_id": event_id,
        "platform": platform,
        "category": category_name,
        "event_type": event_type,
        "signal_level": cause.get("signal_level", ""),
        "stats": cause.get("stats", {}),
        "top_clues": cause.get("clues", []),
        "market": cause.get("market", {}),
        "search_results": "",
        "final_report": "",
    }
    stats = cause.get("stats", {})
    signal_level = cause.get("signal_level", "")
    allow_research, skip_reason = should_research(signal_level, event_type, stats)
    print(f"[Agent] [1/4] event_id={event_id} {platform}/{category_name} signal_level={signal_level}")
    print(f"[Agent] [2/4] event_id={event_id} ë¦¬ì„œì¹˜ ì—¬ë¶€: {'ì§„í–‰' if allow_research else 'ìŠ¤í‚µ'} reason={skip_reason or '-'}")
    if not allow_research:
        update_event(
            event_id,
            "SKIPPED",
            "NONE",
            "",
            "",
            [],
            "",
            "",
            {},
            [],
            "",
            "",
            "NO_RESEARCH",
            skip_reason,
            [],
            "",
        )
        return

    print(f"[Agent] [3/4] event_id={event_id} LLM í˜¸ì¶œ ì¤‘ (OpenAI/graph)...")
    try:
        result = agent_app.invoke(inputs)
    except Exception as e:
        print(f"[Agent] [3/4] event_id={event_id} LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        raise
    verdict = result.get("analysis_verdict", "")
    print(f"[Agent] [3/4] event_id={event_id} LLM ì™„ë£Œ verdict={verdict}")
    update_event(
        event_id,
        "DONE",
        result.get("analysis_tier", "T1"),
        result.get("final_report", ""),
        result.get("spike_reason", ""),
        result.get("entity_keywords", []),
        result.get("cache_key", ""),
        result.get("search_results", ""),
        result.get("watcher_output", {}),
        result.get("search_queries", []),
        result.get("analyst_output", ""),
        result.get("editor_output", ""),
        result.get("analysis_verdict", ""),
        result.get("analysis_reason", ""),
        result.get("evidence_keywords", []),
        result.get("event_kind", ""),
    )
    signal_level = cause.get("signal_level", "")
    verdict = result.get("analysis_verdict", "")
    event_keywords = result.get("event_keywords", []) or []
    growth_ratio = float(stats.get("growth_ratio") or 0.0)
    actual_delta = int(stats.get("delta") or 0)
    is_major = bool(stats.get("major_category"))
    min_alert_delta = ALERT_MAJOR_MIN_DELTA if is_major else ALERT_MIN_DELTA
    min_alert_growth = ALERT_MAJOR_MIN_GROWTH if is_major else ALERT_MIN_GROWTH
    passes_alert_gate = actual_delta > 0 and actual_delta >= min_alert_delta
    should_alert = False
    if AGENT_ALERT_MODE == "all":
        should_alert = passes_alert_gate
    elif AGENT_ALERT_MODE == "confirmed":
        if verdict == "CONFIRMED" and passes_alert_gate:
            should_alert = True
        elif (
            signal_level == "CANDIDATE"
            and passes_alert_gate
            and any(kw in event_keywords for kw in ALERT_KEYWORDS)
        ):
            should_alert = True

    print(f"[Agent] [4/4] event_id={event_id} ê²°ê³¼Â·ì•Œë¦¼: verdict={verdict} gate={passes_alert_gate} should_alert={should_alert} â†’ {'ë°œì†¡' if should_alert else 'ìŠ¤í‚µ'}")
    if should_alert:
        try:
            stats = cause.get("stats", {})
            current = int(stats.get("current") or 0)
            baseline = int(stats.get("baseline_season") or 0)
            delta = int(stats.get("delta") or 0)
            reason = result.get("analysis_reason") or result.get("final_report", "")
            event_kind = result.get("event_kind") or ""
            top_clues = cause.get("clues", []) or []
            top_streamer = top_clues[0].get("name", "Unknown") if top_clues else "Unknown"
            msg = (
                f"ğŸš¨ **[ê¸‰ë“± í™•ì •] {platform}**\n"
                f"ì¹´í…Œê³ ë¦¬: `{category_name}`\n"
                f"í˜„ì¬ ì‹œì²­ì: {current:,}ëª…\n"
                f"ì¦ê°€ëŸ‰: +{delta:,}ëª…\n"
                f"ê¸°ì¤€ ì‹œì²­ì: {baseline:,}ëª…\n"
                f"í•µì‹¬ ìŠ¤íŠ¸ë¦¬ë¨¸: {top_streamer}\n"
                f"íŒì •: {verdict} {event_kind}".strip()
            )
            if reason:
                msg = f"{msg}\nê·¼ê±°: {reason}"
            send_telegram_message(msg, raise_on_failure=True)
            print(f"[Agent] ğŸš¨ [4/4] ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ event_id={event_id} {platform}/{category_name}")
        except Exception as e:
            logging.exception("âŒ [Agent Alert] í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨ (event_id=%s, cat=%s): %s", event_id, category_name, e)
    else:
        print(f"[Agent] [4/4] ì•Œë¦¼ ìŠ¤í‚µ event_id={event_id} verdict={verdict} mode={AGENT_ALERT_MODE}")

def _startup_checks():
    """EC2 ì´ê´€ í›„ ì•ŒëŒ ì•ˆ ì˜¬ ë•Œ ì›ì¸ íŒŒì•…ìš©: PostgresÂ·í•„ìˆ˜ env ê²€ì‚¬."""
    errors = []
    try:
        conn = get_pg_conn()
        conn.close()
        print("[Agent Worker] âœ… Postgres ì—°ê²° OK")
    except Exception as e:
        errors.append(f"Postgres ì—°ê²° ì‹¤íŒ¨: {e}")
        print(f"[Agent Worker] âŒ Postgres ì—°ê²° ì‹¤íŒ¨: {e}")

    for key in ("OPENAI_API_KEY", "TELEGRAM_TOKEN", "TELEGRAM_CHAT_ID"):
        val = os.getenv(key)
        status = "set" if (val and val.strip()) else "NOT SET"
        print(f"[Agent Worker] {key}: {status}")
        if key == "OPENAI_API_KEY" and not (val and val.strip()):
            errors.append("OPENAI_API_KEY ë¹„ì–´ ìˆìŒ")

    if errors:
        print("[Agent Worker] âš ï¸ ì‹œì‘ ê²€ì‚¬ ì´ìƒ:", "; ".join(errors))


def run_worker(poll_interval=5):
    _startup_checks()
    logging.info("ğŸ¤– [Agent Worker] ì‹œì‘ (poll=%ss)", poll_interval)
    poll_count = 0
    while True:
        try:
            rows = fetch_pending()
            if not rows:
                poll_count += 1
                if poll_count % 24 == 0:
                    print(f"[Agent] ëŒ€ê¸° ì¤‘ (PENDING 0ê±´, {poll_interval}ì´ˆë§ˆë‹¤ í´)")
                time.sleep(poll_interval)
                continue
            poll_count = 0
            print(f"[Agent] === PENDING {len(rows)}ê±´ ì²˜ë¦¬ ì‹œì‘ ===")
            for row in rows:
                try:
                    process_event(row)
                except Exception as e:
                    logging.exception("âŒ [Agent Worker] ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨ (event_id=%s): %s", row[0], e)
                    mark_failed(row[0], e)
        except Exception as e:
            logging.exception("âŒ [Agent Worker] ì²˜ë¦¬ ì‹¤íŒ¨: %s", e)
            time.sleep(poll_interval)

if __name__ == "__main__":
    run_worker()
